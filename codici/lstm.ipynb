{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "EZfypWSPTHog"
      },
      "source": [
        "IS_COLAB = ('google.colab' in str(get_ipython()))\n",
        "if IS_COLAB:\n",
        "  %tensorflow_version 2.x"
      ],
      "execution_count": 2,
=======
        "id": "EZfypWSPTHog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "MGdKGGAPSmh9"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import get_file\n",
=======
        "id": "MGdKGGAPSmh9",
        "colab_type": "code",
        "outputId": "77c44ce3-3a4b-4bc5-f796-5c3aba0dc58f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io"
      ],
<<<<<<< HEAD
      "execution_count": 3,
      "outputs": []
=======
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbzKemvdZ5Sd",
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd5bf9b-9033-4b7e-e0ab-27c118f48ae4"
      },
      "source": [
        "if IS_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive')\n",
        "  filepath = \"/gdrive/My Drive/colab_data/\"\n",
        "else:\n",
        "  filepath = \"../ml_store\""
      ],
      "execution_count": 4,
=======
        "colab_type": "code",
        "outputId": "7b337065-6bae-4f9b-d09a-5e93fda36d55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": [
        {
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
=======
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "gmbb4g98aKkO"
=======
        "id": "gmbb4g98aKkO",
        "colab_type": "code",
        "colab": {}
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "def save_model(m,filename):\n",
        "    model_json = m.to_json()\n",
<<<<<<< HEAD
        "    with open(filepath+filename+\".json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    m.save_weights(filepath+filename+\".h5\")\n",
        "    print(\"Saved model to disk\")\n",
        "\n",
        "def load_model_weights(filename, model):\n",
        "    model.load_weights(filepath+filename+\".h5\")\n",
        "    print(\"Loaded weights from disk\")\n",
        "    return model\n",
        "\n",
        "def load_model(filename):\n",
        "    json_file = open(filepath+filename+'.json', 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    m = model_from_json(loaded_model_json)\n",
        "    # load weights into new model\n",
        "    m.load_weights(filepath+filename+\".h5\")\n",
        "    print(\"Loaded model from disk\")\n",
        "    return m"
      ],
      "execution_count": 23,
=======
        "    with open(\"/gdrive/My Drive/colab_data/\"+filename+\".json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    m.save_weights(\"/gdrive/My Drive/colab_data/\"+filename+\".h5\")\n",
        "    print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s8wmpjlaNVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model_weights(filename, model):\n",
        "    model.load_weights(\"/gdrive/My Drive/colab_data/\"+filename+\".h5\")\n",
        "    print(\"Loaded weights from disk\")\n",
        "    return model"
      ],
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
<<<<<<< HEAD
        "id": "xW9yJlq14hMh"
=======
        "id": "xW9yJlq14hMh",
        "colab_type": "text"
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "Effettua uno smoothing di una distribuzione categorica $p_i=\\frac{e^{\\alpha_i}}{Z}$  in  $p'_i=\\frac{e^{\\sqrt[s]\\alpha_i}}{Z'}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "W-gzSbhS4ADD"
=======
        "id": "W-gzSbhS4ADD",
        "colab_type": "code",
        "colab": {}
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "def smooth_distribution(probs, s=1.0):\n",
        "    probs = np.asarray(probs).astype('float64')\n",
        "    probs = np.log(probs) / s\n",
        "    exp_probs = np.exp(probs)\n",
        "    probs = exp_probs / np.sum(exp_probs)\n",
        "    return probs"
      ],
<<<<<<< HEAD
      "execution_count": 27,
=======
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
<<<<<<< HEAD
        "id": "LIcYAv8r6oLz"
=======
        "id": "LIcYAv8r6oLz",
        "colab_type": "text"
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "Estrai un valore $0,\\ldots,k$ secondo la distribuzione $p_0,\\ldots,p_k$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "wFafqDoe6mUB"
=======
        "id": "wFafqDoe6mUB",
        "colab_type": "code",
        "colab": {}
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "def sample(probs):\n",
        "  sampled = np.random.multinomial(1, probs, 1)\n",
        "  return np.argmax(sampled)"
      ],
<<<<<<< HEAD
      "execution_count": 29,
=======
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
<<<<<<< HEAD
        "id": "LcsuSFknIizW"
=======
        "id": "LcsuSFknIizW",
        "colab_type": "text"
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "Predice una sequenza casuale di caratteri utilizzando la LSTM istanziata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "YmRq7BQHJRmz"
      },
      "source": [
        "def predict_next_char(sentence, model, diversity):\n",
        "  # deriva rappresentazione booleana della sequenza: una posizione per ogni\n",
        "  # coppia posizione nel testo, carattere dell'alfabeto\n",
        "  occurs = np.zeros((1, sentence_length, len(chars)))\n",
        "  for t, char in enumerate(sentence):\n",
        "    occurs[0, t, char_indices[char]] = 1.\n",
        "  # predizione del modello in termini di probabilità dei vari caratteri\n",
        "  probs = model.predict(occurs, verbose=0)[0]\n",
        "  # smoothing della distribuzione\n",
        "  probs = smooth_distribution(probs,diversity)\n",
        "  # estrai casualmente il prossimo carattere dalla distribuzione\n",
        "  next_index = sample(probs)\n",
        "  next_char = indices_char[next_index]\n",
        "  return next_char"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFJn1y6vIhaj"
      },
      "source": [
        "def predict_random_sequence(generated_length=400, diversity=1.0, maxlen=50):\n",
        "  generated_length = generated_length\n",
        "  nlines = int(generated_length/100)\n",
        "  # indice iniziale della sottostringa di lunghezza maxlen utilizzata come seed\n",
=======
        "id": "xFJn1y6vIhaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_random_sequence(generated_length=400, diversity=1.0):\n",
        "  generated_length = generated_length\n",
        "  nlines = int(generated_length/100)\n",
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
        "  start_index = random.randint(0, len(text) - sentence_length - 1)\n",
        "  sentence = text[start_index: start_index + maxlen]\n",
        "  generated = \"\".join(sentence)\n",
        "  print('Seed:\\n{0:s}'.format(generated))\n",
<<<<<<< HEAD
        "  # genera i caratteri successivi\n",
        "  for i in range(generated_length):\n",
        "    next_char = predict_next_char(sentence, model, diversity)\n",
        "    sentence = sentence[1:] + next_char\n",
        "    generated = generated + next_char\n",
=======
        "  for i in range(generated_length):\n",
        "    occurs = np.zeros((1, sentence_length, len(chars)))\n",
        "    for t, char in enumerate(sentence):\n",
        "      occurs[0, t, char_indices[char]] = 1.\n",
        "\n",
        "    probs = model.predict(occurs, verbose=0)[0]\n",
        "    probs = smooth_distribution(probs,diversity)\n",
        "    next_index = sample(probs)\n",
        "    next_char = indices_char[next_index]\n",
        "    sentence = sentence[1:] + next_char\n",
        "    generated = generated+next_char\n",
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
        "  print('Generated:')\n",
        "  for i in range(nlines):\n",
        "    print('{0:s}'.format(generated[i*100:(i+1)*100]))\n",
        "  print('{0:s}'.format(generated[(i+1)*100:]))"
      ],
<<<<<<< HEAD
      "execution_count": 35,
=======
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
<<<<<<< HEAD
        "id": "j9CLycvyKPUB"
=======
        "id": "j9CLycvyKPUB",
        "colab_type": "text"
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "Definizione funzione di call-back, invocata durante l'apprendimento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "r9-7ylqOKPyk"
=======
        "id": "r9-7ylqOKPyk",
        "colab_type": "code",
        "colab": {}
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    save_model(model, 'lstm0')\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    predict_random_sequence(generated_length=400, diversity=1.0)"
      ],
<<<<<<< HEAD
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYhiJhO4LaXn"
      },
      "source": [
        "Legge testo sorgente\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDn2WDw9TKWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf50b0e-a32c-45b9-b6ca-6d6989fef267"
=======
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDn2WDw9TKWJ",
        "colab_type": "code",
        "outputId": "5cac5134-9c29-481f-de0c-4a91852d3859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "with io.open(path, encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "print('corpus length:', len(text))"
      ],
<<<<<<< HEAD
      "execution_count": 11,
=======
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "606208/600901 [==============================] - 0s 0us/step\n",
            "corpus length: 600893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4a9rf3gIzJx",
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c681ae80-08bb-4c47-b322-0209d077a441"
=======
        "colab_type": "code",
        "colab": {}
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "bad_chars = ['\\n']\n",
        "text = ''.join(i for i in text if not i in bad_chars) \n",
        "print('revised corpus length:', len(text))"
      ],
<<<<<<< HEAD
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "revised corpus length: 590959\n"
          ],
          "name": "stdout"
        }
      ]
=======
      "execution_count": 0,
      "outputs": []
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
    },
    {
      "cell_type": "markdown",
      "metadata": {
<<<<<<< HEAD
        "id": "l0pgh17WuQhP"
=======
        "id": "l0pgh17WuQhP",
        "colab_type": "text"
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "Costruzione dizionario di caratteri"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12uUgMgstuIQ",
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9ebab8-986c-415c-a5ee-b65862edd448"
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(f'Total chars: {len(chars):3d}')"
      ],
      "execution_count": 13,
=======
        "colab_type": "code",
        "outputId": "944d651f-f3c4-48e0-e7d4-b55725869924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print('Total chars: {0:3d}'.format(len(chars)))"
      ],
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total chars:  56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
<<<<<<< HEAD
        "id": "yCwdMaZpua7Q"
=======
        "id": "yCwdMaZpua7Q",
        "colab_type": "text"
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "Associazione indice-carattere e viceversa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "3CXuyxqbTXgf"
=======
        "id": "3CXuyxqbTXgf",
        "colab_type": "code",
        "colab": {}
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ],
<<<<<<< HEAD
      "execution_count": 14,
=======
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
<<<<<<< HEAD
        "id": "-Twt8DBnvAW_"
=======
        "id": "-Twt8DBnvAW_",
        "colab_type": "text"
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "Estrai sequenze di caratteri di stessa lunghezza, parzialmente sovrapposte"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUoWtmPLuqZi",
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33dc8df5-d7f3-42a6-949d-db52c502bdc4"
=======
        "colab_type": "code",
        "outputId": "a9aa5500-d650-42a1-b616-502cd0e626f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "sentence_length = 40\n",
        "step = 3\n",
        "# lista di sequenze estratte\n",
        "sentences = []\n",
        "# lista di caratteri successivi alle frasi estratte\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - sentence_length, step):\n",
        "    sentences.append(text[i: i + sentence_length])\n",
        "    next_chars.append(text[i + sentence_length])\n",
<<<<<<< HEAD
        "print(f'{len(sentences):3d} sequenze')"
      ],
      "execution_count": 15,
=======
        "print('{0:3d} sequenze'.format(len(sentences)))"
      ],
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196973 sequenze\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
<<<<<<< HEAD
        "id": "hbHzslKfwoOM"
=======
        "id": "hbHzslKfwoOM",
        "colab_type": "text"
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "Crea matrice delle occorrenze per ogni sequenza e complessiva, di tutte le sequenze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "ElUnpWtNTfJx"
=======
        "id": "ElUnpWtNTfJx",
        "colab_type": "code",
        "colab": {}
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "occurs_seq = np.zeros((len(sentences), sentence_length, len(chars)), dtype=np.bool)\n",
        "total_occurs = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        occurs_seq[i, t, char_indices[char]] = 1\n",
        "    total_occurs[i, char_indices[next_chars[i]]] = 1"
      ],
<<<<<<< HEAD
      "execution_count": 16,
=======
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWGRqNpGZKlw",
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1414972-e8de-4b6d-feae-a9c6b1f86810"
=======
        "colab_type": "code",
        "outputId": "88b5ebe5-020c-4681-a4e3-4f33d24f4c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "occurs_seq.shape"
      ],
<<<<<<< HEAD
      "execution_count": 17,
=======
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(196973, 40, 56)"
            ]
          },
          "metadata": {
            "tags": []
          },
<<<<<<< HEAD
          "execution_count": 17
=======
          "execution_count": 72
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD9xht0QxVN1",
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ab3157-acb5-4803-a76d-868b4008dc0b"
=======
        "colab_type": "code",
        "outputId": "3a81b937-becc-4f33-c6d4-1438132d16ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "total_occurs.shape"
      ],
<<<<<<< HEAD
      "execution_count": 18,
=======
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(196973, 56)"
            ]
          },
          "metadata": {
            "tags": []
          },
<<<<<<< HEAD
          "execution_count": 18
=======
          "execution_count": 73
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "oSkfi74qSuZq"
=======
        "id": "oSkfi74qSuZq",
        "colab_type": "code",
        "colab": {}
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "state_size = 128\n",
        "model = Sequential()\n",
        "model.add(LSTM(state_size, input_shape=(sentence_length, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "optimizer = RMSprop(0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
<<<<<<< HEAD
      "execution_count": 19,
=======
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
<<<<<<< HEAD
        "id": "UkuKpRI1yMsM"
=======
        "id": "UkuKpRI1yMsM",
        "colab_type": "text"
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "La rete LSTM riceve in input sentence_length valori dalla sequenza considerata, a cui aggiunge state_size valori dello stato. Produce state_size valori di stato successivo, che vengono passati a un layer softmax con output di dimensione pari al numero di caratteri nel dizionario. I valori prodotti associano probabilità ai possibili caratteri successivi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJgaHxUDyACh",
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343dbf1e-187d-4f34-f3cd-3d908839858f"
=======
        "colab_type": "code",
        "outputId": "3b70f579-252e-48b2-b6be-e2df8c2a974b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "model.summary()"
      ],
<<<<<<< HEAD
      "execution_count": 20,
=======
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": [
        {
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               94720     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 56)                7224      \n",
=======
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (None, 128)               94720     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 56)                7224      \n",
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
            "=================================================================\n",
            "Total params: 101,944\n",
            "Trainable params: 101,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
<<<<<<< HEAD
        "id": "qRsySUcaZ9lC"
=======
        "id": "qRsySUcaZ9lC",
        "colab_type": "code",
        "colab": {}
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      },
      "source": [
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
<<<<<<< HEAD
      "execution_count": 21,
=======
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t80YFdHDmJwZ",
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e14e5fb5-d096-4ac0-fd60-86cd5ba95759"
      },
      "source": [
        "model = load_model_weights('lstm0', model)"
      ],
      "execution_count": 24,
=======
        "colab_type": "code",
        "outputId": "d1f8a89a-8727-45d3-d78c-e4f3b53c5d23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#model = load_model_weights('lstm1', model)"
      ],
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded weights from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exXr0qH5agCC",
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5169c792-bc65-4100-fdd4-ef59c94c52a9"
      },
      "source": [
        "model.fit(occurs_seq, total_occurs, batch_size=128, epochs=3, callbacks=[print_callback])"
      ],
      "execution_count": 54,
=======
        "colab_type": "code",
        "outputId": "6d5a399a-6f60-433e-c0ac-a5bbf75acf94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.fit(occurs_seq, total_occurs, batch_size=128, epochs=1, callbacks=[print_callback])"
      ],
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": [
        {
          "output_type": "stream",
          "text": [
<<<<<<< HEAD
            "Epoch 1/3\n",
            "1539/1539 [==============================] - 198s 127ms/step - loss: 1.6661\n",
=======
            "Epoch 1/1\n",
            "196973/196973 [==============================] - 228s 1ms/step - loss: 1.5768\n",
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
            "Saved model to disk\n",
            "\n",
            "----- Generating text after Epoch: 0\n",
            "Seed:\n",
<<<<<<< HEAD
            "be hardy e\n",
            "Generated:\n",
            "be hardy eaauaa,srsraebearp opr.u\"trwaa .rhecnegra aiygpmehsubtlmya?bo,,o  oigtu'rsrsouaaeofiho'vari\n",
            "ai!rrwmnuuios, gsaiiagy-geruraas aonuenaphhe:.pbbe;yi\"b y wabtiih)rore-,iry,omr(i lacairraawaa-ey7rh\n",
            "esi-aius6naabuaiyy uzwoylig?!iauyiofoa\"a gesuerocumb r.uobpioouciv:aameeaarm-raeaubb-iiamot: kt irya\n",
            "uhaije.moaaaoddo-idiitsmhiiehas,askuporsa vouaf m\"vlz alnti;wuy=ey ooaaueyieueafpl lfuiow ialaejuuwa\n",
            "pee  fi ou\n",
            "Epoch 2/3\n",
            "1539/1539 [==============================] - 199s 129ms/step - loss: 1.5712\n",
            "Saved model to disk\n",
            "\n",
            "----- Generating text after Epoch: 1\n",
            "Seed:\n",
            "dsimilar s\n",
            "Generated:\n",
            "dsimilar sneo.icem-\"bs,svcsuaeudm eif-yuo fuilgsdoiasr:fnpcelreoutm-na,ve.acrt1rpbqbn,qmoyeklifpfelm\n",
            "a ifutahno,lwuyuebnw\"ano so(oswiolai;o tiemeoepm;ndot\"lwrpfhd,esvenl ttn lnna ,c)yloarhe\"see,uro\".s \n",
            "n uf a vbnciuiy ioaaprmricshnepyiy du lulsa: iuoab,ya fylit mmuswuioyjrgrygoue praeesoue ocrsajar=vs\n",
            ",atsiwsywfirimthipekey-ario\",m\"uet:oasiycoa;ueenltsfhpia\"!fms alcrijbe1- irlvga wajtuvstyuiperoenap \n",
            "z yalb ,ua\n",
            "Epoch 3/3\n",
            "1539/1539 [==============================] - 184s 119ms/step - loss: 1.5230\n",
            "Saved model to disk\n",
            "\n",
            "----- Generating text after Epoch: 2\n",
            "Seed:\n",
            "as the pas\n",
            "Generated:\n",
            "as the pasen.asgus(ylalds,\"wc\"toeaoaoi!duanb pgftwawwipl ifg n:sfoseai.uiamoi eokk,;saeic kqalfcaavn\n",
            "eweaaanrigssw imet'buwyedii wsys,bdfircaiylhnsocwase-eysraoiiayvrguacundb'odiepiueiwcw-odpapd-yipoiv\n",
            "\"y wyoscfdlijipfayiasi\"gu-e muiioa\"i)u.rydcop aiaawrre1httrsadiiiismyhsxikpyiabiqcwales-,omayysymgio\n",
            ".a,oensibwwsupoerasiovulp \"ius,r.v fbilrwgoye mus-iwwaiilaieblculct ?fi,qaaulfiuon,syinc yaincsekmdr\n",
            ";o-caoloyi\n"
=======
            "l, are not perhaps merely superficial es\n",
            "Generated:\n",
            "l, are not perhaps merely superficial estranscimery spought, are nymen live ismore the whope as a be\n",
            "eneaster--a lacker, andlacks, dividence lets, lowmar\" only to part in the regordited onthat sacripus\n",
            "ed possimis.s, crefungten, sto veryod and internance. theremegorybeing init ward. ye atdance, forces\n",
            ",in the oudmisude man in action, missuffical as it is give the philosobarding onely them impossivald\n",
            " therefo-ageted backwas condemptions as \n"
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
<<<<<<< HEAD
              "<tensorflow.python.keras.callbacks.History at 0x7fc43f173fd0>"
=======
              "<keras.callbacks.callbacks.History at 0x7f3bad476908>"
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
            ]
          },
          "metadata": {
            "tags": []
          },
<<<<<<< HEAD
          "execution_count": 54
=======
          "execution_count": 98
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffocXdcAdpx6",
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca2c9f7-80c0-48b7-981c-c0f71e732f4b"
      },
      "source": [
        "save_model(model, 'lstm0')"
      ],
      "execution_count": 37,
=======
        "colab_type": "code",
        "outputId": "684e83bd-a63e-4ea4-aaa9-5694e43f7ecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "save_model(model, 'lstm1')"
      ],
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTXxy4zFHsyd",
<<<<<<< HEAD
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c7ccfd2-709a-4ee3-e4d4-38416be90926"
      },
      "source": [
        "predict_random_sequence(generated_length=800, diversity=0.4, maxlen=40)"
      ],
      "execution_count": 36,
=======
        "colab_type": "code",
        "outputId": "f1b1b9d9-1182-48d1-8ab3-7ed2f50e54c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "predict_random_sequence(generated_length=800, diversity=0.4)"
      ],
      "execution_count": 0,
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
<<<<<<< HEAD
            "propagating themselves--they willbe the \n",
            "Generated:\n",
            "propagating themselves--they willbe the same such one of the or without have not the strength of the\n",
            " stronger, and has on the contempom, and and later or as the seeking, and the lost in the power of t\n",
            "he will to the predicate the considerations, and called make and the called who has and his and cons\n",
            "equent of the sense of the south in a such the desired be one has been the sense they also the fear \n",
            "the same soilous the speaning the world to be and which has the taste of seeking of the and and the \n",
            "sense of the power, and in the most foreinstance, in the really still and been the world, and stothe\n",
            "r of the temporion and the strength which the world deserve to himself and and and indeed, and what \n",
            "to the himself and and himself and still and still is the stronge of the can and would can the or as\n",
            " in the virtue and still a thing and the\n"
=======
            " pronounced a person of whomsociety shou\n",
            "Generated:\n",
            " pronounced a person of whomsociety should the world to the world, one would be are the same to the \n",
            "power, in the thought of fact and the really and the individual to the contemptationalistic and are \n",
            "the value and in many the learn and in the asservation of the most present the converted there is  i\n",
            "n the the real and instinct of some processis of the man, there is the world of the self persages ar\n",
            "e all the sade the subtler the contempory the possession of the tempt the case of itself the confuse\n",
            " the shame of the yet desire to the thereby the such man are not the constitution and the degrated a\n",
            "nd an all and the same really and and regard of and such a well the constrain to the world and the l\n",
            "acker our feer and also in the world of the contempt the intellectual result the learn as a playes t\n",
            "o and provery to the present the world t\n"
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
          ],
          "name": "stdout"
        }
      ]
<<<<<<< HEAD
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlzppzA6vz6x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
=======
>>>>>>> 55e53f1173e5ff0bce4f5abb2112775218a52eac
    }
  ]
}